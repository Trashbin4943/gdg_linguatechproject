# Baseline ì„¤ê³„ ë° Fine-tuning ê°€ì´ë“œ

## ğŸ“‹ ëª©ì°¨
1. [Baseline ì„¤ê³„ ê°œìš”](#baseline-ì„¤ê³„-ê°œìš”)
2. [Baseline ì•„í‚¤í…ì²˜](#baseline-ì•„í‚¤í…ì²˜)
3. [Fine-tuningì„ ìœ„í•œ ë°ì´í„°ì…‹ êµ¬ì¡°](#fine-tuningì„-ìœ„í•œ-ë°ì´í„°ì…‹-êµ¬ì¡°)
4. [ë°ì´í„° í†µí•© ê³¼ì •](#ë°ì´í„°-í†µí•©-ê³¼ì •)
5. [KoBERT Fine-tuning êµ¬í˜„](#kobert-fine-tuning-êµ¬í˜„)

---

## Baseline ì„¤ê³„ ê°œìš”

### ğŸ¯ ì„¤ê³„ ëª©í‘œ
- **ë¹ ë¥¸ í”„ë¡œí† íƒ€ì…**: ì œì¶œ ê¸°í•œ(11/28) ë‚´ ì‘ë™í•˜ëŠ” ê²°ê³¼ë¬¼ í™•ë³´
- **í™•ì¥ ê°€ëŠ¥ì„±**: ì´í›„ ML ëª¨ë¸ í†µí•©ì´ ìš©ì´í•œ êµ¬ì¡°
- **ì‹¤ìš©ì„±**: ì‹¤ì œ ìƒë‹´ ë°ì´í„°ì— ë°”ë¡œ ì ìš© ê°€ëŠ¥

### ğŸ“Š Baseline êµ¬ì„± ìš”ì†Œ

#### 1. **í‚¤ì›Œë“œ ê¸°ë°˜ ê·œì¹™ ì—”ì§„** (Rule-based)
- **ìœ„ì¹˜**: `classification_criteria.py`
- **ë°©ì‹**: ì •ê·œí‘œí˜„ì‹ + í‚¤ì›Œë“œ ë§¤ì¹­
- **ì¹´í…Œê³ ë¦¬**: 11ê°œ ì¹´í…Œê³ ë¦¬ + ì •ìƒ ì¹´í…Œê³ ë¦¬
  - ìš•ì„¤_ì €ì£¼, ëª¨ìš•_ì¡°ë¡±, í­ë ¥_ìœ„í˜‘_ë²”ì£„ì¡°ì¥, ì™¸ì„¤_ì„±í¬ë¡±, í˜ì˜¤í‘œí˜„
  - ë°˜ë³µì„±, ë¬´ë¦¬í•œ_ìš”êµ¬, ë¶€ë‹¹ì„±, í—ˆìœ„_ë¯¼ì›, ì¥ë‚œì „í™”, ê³µí¬ì‹¬_ë¶ˆì•ˆê°_ìœ ë°œ
- **ì¥ì **: 
  - ì¦‰ì‹œ ì‘ë™, ì¶”ê°€ í•™ìŠµ ë¶ˆí•„ìš”
  - ëª…í™•í•œ íŒë‹¨ ê·¼ê±° ì œê³µ (ì„¤ëª… ê°€ëŠ¥ì„±)
  - ë¹ ë¥¸ ì‹¤í–‰ ì†ë„
- **ë‹¨ì **:
  - ìƒˆë¡œìš´ í‘œí˜„ íŒ¨í„´ ê°ì§€ ì–´ë ¤ì›€
  - ì™„ê³¡ í‘œí˜„/ì€ì–´ ì²˜ë¦¬ í•œê³„
  - ë§¥ë½ ì´í•´ ë¶€ì¡±

#### 2. **ì„¸ì…˜ ë§¥ë½ ë¶„ì„** (Context-aware)
- **ê¸°ëŠ¥**: ì´ì „ ëŒ€í™”ì™€ ë¹„êµí•˜ì—¬ ë°˜ë³µì„± ê°ì§€
- **ë°©ì‹**: ê°„ë‹¨í•œ í‚¤ì›Œë“œ ìœ ì‚¬ë„ + ë°˜ë³µ í‘œí˜„ íŒ¨í„´
- **í–¥ìƒ ë°©ì•ˆ**: ë¬¸ì¥ ì„ë² ë”© ìœ ì‚¬ë„ë¡œ ê°œì„  ì˜ˆì •

#### 3. **ë‹¤ì¤‘ ì¹´í…Œê³ ë¦¬ ê°ì§€** (Multi-label)
- í•œ í…ìŠ¤íŠ¸ì—ì„œ ì—¬ëŸ¬ ë¬¸ì œ ë™ì‹œ ê°ì§€
- ì˜ˆ: "XíŒ” ë„ˆ ê±°ê¸°ì„œ ë­ ë°°ì› ëƒ" â†’ `ìš•ì„¤_ì €ì£¼` + `ëª¨ìš•_ì¡°ë¡±`

#### 4. **ì‹¬ê°ë„ ê¸°ë°˜ ì¡°ì¹˜ ì œì•ˆ** (Severity-based Action)
- 5ë‹¨ê³„ ì‹¬ê°ë„ ë ˆë²¨
- ê° ë ˆë²¨ë³„ ìë™ ì¡°ì¹˜ ë°©ì•ˆ ì œì‹œ

---

## Baseline ì•„í‚¤í…ì²˜

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    ì…ë ¥ í…ìŠ¤íŠ¸                            â”‚
â”‚              (STT ê²°ê³¼ ë˜ëŠ” ì§ì ‘ ì…ë ¥)                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
                     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚          ClassificationCriteria.classify_text()         â”‚
â”‚                                                          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚  í‚¤ì›Œë“œ ë§¤ì¹­  â”‚  â”‚  ì„¸ì…˜ ë§¥ë½   â”‚  â”‚  íŒ¨í„´ ë¶„ì„   â”‚ â”‚
â”‚  â”‚  (9ê°œ ì¹´í…Œê³ ë¦¬)â”‚  â”‚  (ë°˜ë³µì„±)    â”‚  â”‚  (ì‹¬ê°ë„)    â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                                                          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  ClassificationResult ë¦¬ìŠ¤íŠ¸ ìƒì„±                 â”‚  â”‚
â”‚  â”‚  - category: ComplaintCategory                   â”‚  â”‚
â”‚  â”‚  - severity: ComplaintSeverity                   â”‚  â”‚
â”‚  â”‚  - confidence: float (0.0~1.0)                   â”‚  â”‚
â”‚  â”‚  - evidence: List[str] (íŒë‹¨ ê·¼ê±°)                â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
                     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              ë‹¤ì¤‘ ì¹´í…Œê³ ë¦¬ ê²°ê³¼                          â”‚
â”‚  [ìš•ì„¤_ì €ì£¼(HIGH), ëª¨ìš•_ì¡°ë¡±(MEDIUM), ...]              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Baseline í‚¤ì›Œë“œ í™•ì¥

ë…¸íŠ¸ë¶ êµ¬í˜„ì—ì„œëŠ” ë‹¤ìŒ í‚¤ì›Œë“œë“¤ì´ ì¶”ê°€ë¡œ í¬í•¨ë˜ì–´ ìˆìŠµë‹ˆë‹¤:

- **ìš•ì„¤ í‚¤ì›Œë“œ ì¶”ê°€**: "ë‹¥ì³", "ì—¿ë¨¹ì–´", "ê°œì†Œë¦¬"
- **ëª¨ìš• í‚¤ì›Œë“œ ì¶”ê°€**: "ì“°ë ˆê¸°", "í•œì‹¬í•œ", "ì–´ë¦¬ì„ì€", "ìˆ˜ì¤€ ë¯¸ë‹¬"
- **ìœ„í˜‘ í‚¤ì›Œë“œ ì¶”ê°€**: "ê°€ë§Œ ì•ˆ ë‘ê² ë‹¤", "í•´ì½”ì§€", "ì‹ ìƒ í„¸ì–´", "ê°€ë§Œë‘ì§€ ì•Šê² ë‹¤"
- **ë°˜ë³µì„± í‚¤ì›Œë“œ ì¶”ê°€**: "ê°™ì€ ë‚´ìš©", "ëª‡ ë²ˆì„ ë§í•´ì•¼", "ë˜‘ê°™ì€ ì´ì•¼ê¸°"

### í˜„ì¬ Baselineì˜ í•œê³„ì 

1. **í‘œí˜„ ë‹¤ì–‘ì„± ë¶€ì¡±**
   - í‚¤ì›Œë“œ ë¦¬ìŠ¤íŠ¸ê°€ ì œí•œì  (ì§€ì†ì  í™•ì¥ í•„ìš”)
   - ì™„ê³¡ í‘œí˜„, ì€ì–´, ì‹ ì¡°ì–´ ë¯¸ì²˜ë¦¬

2. **ë§¥ë½ ì´í•´ ë¶€ì¡±**
   - ë¬¸ë§¥ì— ë”°ë¥¸ ì˜ë¯¸ ë³€í™” ë¯¸ë°˜ì˜
   - ì˜ˆ: "ì£½ì—¬ì¤˜" (ë†ë‹´ vs ì§„ì§€í•œ ìœ„í˜‘)

3. **ë°˜ë³µì„± ê°ì§€ ì •í™•ë„ ë‚®ìŒ**
   - ë‹¨ìˆœ í‚¤ì›Œë“œ ë§¤ì¹­ìœ¼ë¡œëŠ” í•œê³„
   - ì˜ë¯¸ì  ìœ ì‚¬ë„ ì¸¡ì • í•„ìš”

4. **ë¬´ë¦¬í•œ ìš”êµ¬/ë¶€ë‹¹ì„± íŒë‹¨ ì–´ë ¤ì›€**
   - ë„ë©”ì¸ ì§€ì‹(ë§¤ë‰´ì–¼, ê¶Œí•œ) í•„ìš”
   - ê·œì¹™ë§Œìœ¼ë¡œëŠ” íŒë‹¨ í•œê³„

---

## Fine-tuningì„ ìœ„í•œ ë°ì´í„°ì…‹ êµ¬ì¡°

### ğŸ“ ë°ì´í„°ì…‹ í˜•ì‹

#### ì˜µì…˜ 1: CSV í˜•ì‹ (ê¶Œì¥)
```csv
text,label,severity,session_id,turn_id,context
"XíŒ” ë„ˆ ê±°ê¸°ì„œ ë­ ë°°ì› ëƒ?",ìš•ì„¤_ì €ì£¼|ëª¨ìš•_ì¡°ë¡±,HIGH|MEDIUM,session_001,1,
"ì•ì„  í†µí™”ì—ì„œë„ ë§ì”€ë“œë ¸ë‹¤ì‹œí”¼ ê°™ì€ ì–˜ê¸°ì¸ë°ìš”",ë°˜ë³µì„±,MEDIUM,session_001,2,"ì´ì „ ëŒ€í™” ë‚´ìš©"
"ì •ìƒì ì¸ ë¬¸ì˜ì…ë‹ˆë‹¤",ì •ìƒ,NORMAL,session_002,1,
```

#### ì˜µì…˜ 2: JSON í˜•ì‹
```json
{
  "sessions": [
    {
      "session_id": "session_001",
      "turns": [
        {
          "turn_id": 1,
          "speaker": "customer",
          "text": "XíŒ” ë„ˆ ê±°ê¸°ì„œ ë­ ë°°ì› ëƒ?",
          "labels": ["ìš•ì„¤_ì €ì£¼", "ëª¨ìš•_ì¡°ë¡±"],
          "severities": ["HIGH", "MEDIUM"],
          "context": []
        },
        {
          "turn_id": 2,
          "speaker": "customer",
          "text": "ì•ì„  í†µí™”ì—ì„œë„ ë§ì”€ë“œë ¸ë‹¤ì‹œí”¼",
          "labels": ["ë°˜ë³µì„±"],
          "severities": ["MEDIUM"],
          "context": ["ì´ì „ ëŒ€í™”"]
        }
      ]
    }
  ]
}
```

### ğŸ·ï¸ ë¼ë²¨ë§ ì²´ê³„

#### ë‹¤ì¤‘ ë¼ë²¨ (Multi-label) êµ¬ì¡°
- **í•˜ë‚˜ì˜ í…ìŠ¤íŠ¸ì— ì—¬ëŸ¬ ì¹´í…Œê³ ë¦¬ ë™ì‹œ ë¼ë²¨ë§ ê°€ëŠ¥**
- ì˜ˆ: `["ìš•ì„¤_ì €ì£¼", "ëª¨ìš•_ì¡°ë¡±"]`

#### ë¼ë²¨ ë§¤í•‘ (KoBERT ì…ë ¥ìš©)
```python
LABEL_MAPPING = {
    "ì •ìƒ": 0,
    "ìš•ì„¤_ì €ì£¼": 1,
    "ëª¨ìš•_ì¡°ë¡±": 2,
    "í­ë ¥_ìœ„í˜‘_ë²”ì£„ì¡°ì¥": 3,
    "ì™¸ì„¤_ì„±í¬ë¡±": 4,
    "í˜ì˜¤í‘œí˜„": 5,
    "ë°˜ë³µì„±": 6,
    "ë¬´ë¦¬í•œ_ìš”êµ¬": 7,
    "ë¶€ë‹¹ì„±": 8,
    "í—ˆìœ„_ë¯¼ì›": 9,
    "ì¥ë‚œì „í™”": 10,
    "ê³µí¬ì‹¬_ë¶ˆì•ˆê°_ìœ ë°œ": 11
}

# ë‹¤ì¤‘ ë¼ë²¨ì„ ìœ„í•œ ì´ì§„ ë²¡í„°
# ì˜ˆ: ["ìš•ì„¤_ì €ì£¼", "ëª¨ìš•_ì¡°ë¡±"] â†’ [0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]
```

### ğŸ“Š ë°ì´í„°ì…‹ ìš”êµ¬ì‚¬í•­

#### ìµœì†Œ ë°ì´í„° ê·œëª¨
- **ì¹´í…Œê³ ë¦¬ë³„ ìµœì†Œ 100ê°œ ì´ìƒ** (ê¶Œì¥: 500ê°œ ì´ìƒ)
- **ì´ ë°ì´í„°: ìµœì†Œ 1,000ê°œ** (ê¶Œì¥: 5,000ê°œ ì´ìƒ)
- **í´ë˜ìŠ¤ ë¶ˆê· í˜• ê³ ë ¤**: ì†Œìˆ˜ í´ë˜ìŠ¤ë„ ìµœì†Œ 50ê°œ ì´ìƒ

#### ë°ì´í„° ë¶„í• 
```
ì „ì²´ ë°ì´í„° (100%)
â”œâ”€â”€ í•™ìŠµ ë°ì´í„° (train): 70%
â”œâ”€â”€ ê²€ì¦ ë°ì´í„° (val): 15%
â””â”€â”€ í…ŒìŠ¤íŠ¸ ë°ì´í„° (test): 15%
```

#### ì„¸ì…˜ ëˆ„ìˆ˜ ë°©ì§€
- **ê°™ì€ ì„¸ì…˜ì˜ ë°ì´í„°ëŠ” ê°™ì€ splitì—ë§Œ í¬í•¨**
- ì„¸ì…˜ ë‹¨ìœ„ë¡œ ë¶„í•  í•„ìš”

---

## ë°ì´í„° í†µí•© ê³¼ì •

### 1ë‹¨ê³„: ë°ì´í„° ìˆ˜ì§‘ ë° ì •ì œ

```python
# data_preparation.py

import pandas as pd
import json
from typing import List, Dict

def load_raw_data(data_path: str, format: str = "csv"):
    """ì›ë³¸ ë°ì´í„° ë¡œë“œ"""
    if format == "csv":
        df = pd.read_csv(data_path)
    elif format == "json":
        with open(data_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
        df = convert_json_to_dataframe(data)
    return df

def clean_text(text: str) -> str:
    """í…ìŠ¤íŠ¸ ì •ì œ"""
    # ê°œì¸ì •ë³´ ë§ˆìŠ¤í‚¹
    text = mask_pii(text)
    # íŠ¹ìˆ˜ë¬¸ì ì •ê·œí™”
    text = normalize_special_chars(text)
    # ê³µë°± ì •ë¦¬
    text = ' '.join(text.split())
    return text

def convert_labels_to_multilabel(labels: str) -> List[int]:
    """ë¼ë²¨ ë¬¸ìì—´ì„ ë‹¤ì¤‘ ë¼ë²¨ ë²¡í„°ë¡œ ë³€í™˜"""
    label_list = labels.split('|')
    multilabel_vector = [0] * len(LABEL_MAPPING)
    for label in label_list:
        if label in LABEL_MAPPING:
            multilabel_vector[LABEL_MAPPING[label]] = 1
    return multilabel_vector
```

### 2ë‹¨ê³„: Baselineìœ¼ë¡œ ìë™ ë¼ë²¨ë§ (ë¶€ì¡±í•œ ë°ì´í„° ë³´ì™„)

```python
# auto_labeling.py

from classification_criteria import ClassificationCriteria

def auto_label_with_baseline(text: str, session_context: List[str] = None):
    """Baseline ê·œì¹™ìœ¼ë¡œ ìë™ ë¼ë²¨ë§ (ê²€ì¦ìš©)"""
    results = ClassificationCriteria.classify_text(text, session_context)
    
    # ì •ìƒì´ ì•„ë‹Œ ê²°ê³¼ë§Œ ì¶”ì¶œ
    labels = [r.category.value for r in results if r.severity.value != "ì •ìƒ"]
    
    if not labels:
        return ["ì •ìƒ"]
    return labels

def augment_dataset_with_baseline(df: pd.DataFrame):
    """Baselineìœ¼ë¡œ ë¼ë²¨ì´ ì—†ëŠ” ë°ì´í„°ì— ìë™ ë¼ë²¨ë§"""
    df['auto_labels'] = df.apply(
        lambda row: auto_label_with_baseline(row['text'], row.get('context', [])),
        axis=1
    )
    return df
```

### 3ë‹¨ê³„: ë°ì´í„° ê²€ì¦ ë° í’ˆì§ˆ ê´€ë¦¬

```python
# data_validation.py

def validate_dataset(df: pd.DataFrame):
    """ë°ì´í„°ì…‹ í’ˆì§ˆ ê²€ì¦"""
    issues = []
    
    # 1. ë¹ˆ í…ìŠ¤íŠ¸ ì²´í¬
    empty_texts = df[df['text'].str.strip() == '']
    if len(empty_texts) > 0:
        issues.append(f"ë¹ˆ í…ìŠ¤íŠ¸ {len(empty_texts)}ê±´ ë°œê²¬")
    
    # 2. ë¼ë²¨ ë¶„í¬ ì²´í¬
    label_counts = count_labels(df)
    for label, count in label_counts.items():
        if count < 50:
            issues.append(f"ë¼ë²¨ '{label}' ìƒ˜í”Œ ë¶€ì¡±: {count}ê°œ")
    
    # 3. ì„¸ì…˜ ëˆ„ìˆ˜ ì²´í¬
    session_leakage = check_session_leakage(df)
    if session_leakage:
        issues.append("ì„¸ì…˜ ëˆ„ìˆ˜ ë°œê²¬")
    
    return issues

def split_by_session(df: pd.DataFrame, train_ratio=0.7, val_ratio=0.15):
    """ì„¸ì…˜ ë‹¨ìœ„ë¡œ ë°ì´í„° ë¶„í• """
    sessions = df['session_id'].unique()
    n_sessions = len(sessions)
    
    train_sessions = sessions[:int(n_sessions * train_ratio)]
    val_sessions = sessions[int(n_sessions * train_ratio):
                           int(n_sessions * (train_ratio + val_ratio))]
    test_sessions = sessions[int(n_sessions * (train_ratio + val_ratio)):]
    
    train_df = df[df['session_id'].isin(train_sessions)]
    val_df = df[df['session_id'].isin(val_sessions)]
    test_df = df[df['session_id'].isin(test_sessions)]
    
    return train_df, val_df, test_df
```

### 4ë‹¨ê³„: KoBERT ì…ë ¥ í˜•ì‹ ë³€í™˜

```python
# data_conversion.py

from transformers import BertTokenizer

def prepare_kobert_dataset(df: pd.DataFrame, tokenizer: BertTokenizer, max_length=128):
    """KoBERT í•™ìŠµìš© ë°ì´í„°ì…‹ ì¤€ë¹„"""
    
    texts = df['text'].tolist()
    labels = df['multilabel_vector'].tolist()  # ì´ì§„ ë²¡í„° ë¦¬ìŠ¤íŠ¸
    
    # í† í¬ë‚˜ì´ì§•
    encodings = tokenizer(
        texts,
        truncation=True,
        padding=True,
        max_length=max_length,
        return_tensors='pt'
    )
    
    # ë¼ë²¨ì„ í…ì„œë¡œ ë³€í™˜
    import torch
    label_tensors = torch.tensor(labels, dtype=torch.float)
    
    return {
        'input_ids': encodings['input_ids'],
        'attention_mask': encodings['attention_mask'],
        'labels': label_tensors
    }
```

---

## KoBERT Fine-tuning êµ¬í˜„

### ì•„í‚¤í…ì²˜ ì„¤ê³„

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              Baseline (Rule-based)                      â”‚
â”‚         ë¹ ë¥¸ í•„í„°ë§, ëª…í™•í•œ ê·¼ê±° ì œê³µ                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
                     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         KoBERT Fine-tuned Model                         â”‚
â”‚    ë§¥ë½ ì´í•´, ë‹¤ì–‘í•œ í‘œí˜„ íŒ¨í„´ ê°ì§€                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
                     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              Ensemble (í•˜ì´ë¸Œë¦¬ë“œ)                       â”‚
â”‚  Baseline + KoBERT ê²°ê³¼ í†µí•©                            â”‚
â”‚  - Baseline ì‹ ë¢°ë„ ë†’ìœ¼ë©´ Baseline ìš°ì„                   â”‚
â”‚  - ëª¨í˜¸í•œ ê²½ìš° KoBERT ê²°ê³¼ í™œìš©                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Fine-tuning ì½”ë“œ êµ¬ì¡°

```python
# kobert_finetuning.py

import torch
from torch.utils.data import Dataset, DataLoader
from transformers import (
    BertTokenizer, 
    BertForSequenceClassification,
    TrainingArguments,
    Trainer
)
from sklearn.metrics import f1_score, precision_recall_fscore_support
import numpy as np

class ComplaintDataset(Dataset):
    """ë¯¼ì› ë¶„ë¥˜ ë°ì´í„°ì…‹"""
    
    def __init__(self, texts, labels, tokenizer, max_length=128):
        self.texts = texts
        self.labels = labels
        self.tokenizer = tokenizer
        self.max_length = max_length
    
    def __len__(self):
        return len(self.texts)
    
    def __getitem__(self, idx):
        text = str(self.texts[idx])
        label = self.labels[idx]
        
        encoding = self.tokenizer(
            text,
            truncation=True,
            padding='max_length',
            max_length=self.max_length,
            return_tensors='pt'
        )
        
        return {
            'input_ids': encoding['input_ids'].flatten(),
            'attention_mask': encoding['attention_mask'].flatten(),
            'labels': torch.tensor(label, dtype=torch.float)
        }

def compute_metrics(eval_pred):
    """í‰ê°€ ë©”íŠ¸ë¦­ ê³„ì‚° (ë‹¤ì¤‘ ë¼ë²¨)"""
    predictions, labels = eval_pred
    
    # ì‹œê·¸ëª¨ì´ë“œ ì ìš©í•˜ì—¬ ì´ì§„ ë¶„ë¥˜ë¡œ ë³€í™˜
    predictions = torch.sigmoid(torch.tensor(predictions)).numpy()
    predictions = (predictions > 0.5).astype(int)
    
    # ê° ë¼ë²¨ë³„ F1 ì ìˆ˜
    f1_scores = []
    for i in range(labels.shape[1]):
        f1 = f1_score(labels[:, i], predictions[:, i], average='binary', zero_division=0)
        f1_scores.append(f1)
    
    # í‰ê·  F1 ì ìˆ˜
    avg_f1 = np.mean(f1_scores)
    
    return {
        'f1_macro': avg_f1,
        'f1_per_label': {f'label_{i}': f1 for i, f1 in enumerate(f1_scores)}
    }

def train_kobert_multilabel(
    train_df,
    val_df,
    model_name='monologg/kobert',
    num_labels=12,
    output_dir='./kobert_complaint_classifier',
    epochs=3,
    batch_size=16,
    learning_rate=2e-5
):
    """KoBERT ë‹¤ì¤‘ ë¼ë²¨ ë¶„ë¥˜ ëª¨ë¸ í•™ìŠµ"""
    
    # í† í¬ë‚˜ì´ì € ë¡œë“œ
    tokenizer = BertTokenizer.from_pretrained(model_name)
    
    # ëª¨ë¸ ë¡œë“œ (ë‹¤ì¤‘ ë¼ë²¨ ë¶„ë¥˜ìš©)
    model = BertForSequenceClassification.from_pretrained(
        model_name,
        num_labels=num_labels,
        problem_type="multi_label_classification"  # ë‹¤ì¤‘ ë¼ë²¨ ì„¤ì •
    )
    
    # ë°ì´í„°ì…‹ ì¤€ë¹„
    train_dataset = ComplaintDataset(
        train_df['text'].tolist(),
        train_df['multilabel_vector'].tolist(),
        tokenizer
    )
    
    val_dataset = ComplaintDataset(
        val_df['text'].tolist(),
        val_df['multilabel_vector'].tolist(),
        tokenizer
    )
    
    # í•™ìŠµ ì„¤ì •
    training_args = TrainingArguments(
        output_dir=output_dir,
        num_train_epochs=epochs,
        per_device_train_batch_size=batch_size,
        per_device_eval_batch_size=batch_size,
        learning_rate=learning_rate,
        warmup_steps=500,
        weight_decay=0.01,
        logging_dir=f'{output_dir}/logs',
        logging_steps=100,
        evaluation_strategy="epoch",
        save_strategy="epoch",
        load_best_model_at_end=True,
        metric_for_best_model="f1_macro",
    )
    
    # Trainer ì´ˆê¸°í™”
    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=train_dataset,
        eval_dataset=val_dataset,
        compute_metrics=compute_metrics,
    )
    
    # í•™ìŠµ ì‹¤í–‰
    trainer.train()
    
    # ëª¨ë¸ ì €ì¥
    trainer.save_model()
    tokenizer.save_pretrained(output_dir)
    
    return model, tokenizer
```

### Risk Score ê³„ì‚° ë°©ë²•ë¡ 

Risk ScoreëŠ” ì—¬ëŸ¬ ì•…ì„± ìš”ì¸ì„ ì¢…í•©í•˜ì—¬ 0-10ì  ì²™ë„ë¡œ ìœ„í—˜ë„ë¥¼ ì‚°ì •í•©ë‹ˆë‹¤. ë‹¤ì–‘í•œ ê³„ì‚° ë°©ë²•ë¡ ì„ ì œì‹œí•˜ë©°, ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” ë°ì´í„° íŠ¹ì„±ê³¼ ìš”êµ¬ì‚¬í•­ì— ë§ê²Œ ì„ íƒí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

#### ê¸°ë³¸ ì‹¬ê°ë„ ì ìˆ˜ ë§¤í•‘

ëª¨ë“  ë°©ë²•ë¡ ì—ì„œ ê³µí†µìœ¼ë¡œ ì‚¬ìš©í•˜ëŠ” ì‹¬ê°ë„ë³„ ê¸°ë³¸ ì ìˆ˜:

- **CRITICAL**: 4ì 
- **HIGH**: 3ì 
- **MEDIUM**: 2ì 
- **LOW**: 1ì 
- **NORMAL**: 0ì 

#### ë°©ë²•ë¡  1: ì„ í˜• í•©ì‚° ë°©ì‹ (Linear Sum)

ê°€ì¥ ë‹¨ìˆœí•˜ê³  ì§ê´€ì ì¸ ë°©ì‹ìœ¼ë¡œ, ëª¨ë“  ì•…ì„± ìš”ì¸ì˜ ì ìˆ˜ë¥¼ ë‹¨ìˆœ í•©ì‚°í•©ë‹ˆë‹¤.

```python
def calculate_risk_linear(baseline_issues, metadata_issues):
    """ì„ í˜• í•©ì‚° ë°©ì‹"""
    total_score = 0
    
    # Baseline ì´ìŠˆ ì ìˆ˜ í•©ì‚°
    for issue in baseline_issues:
        if 'CRITICAL' in issue: total_score += 4
        elif 'HIGH' in issue: total_score += 3
        elif 'MEDIUM' in issue: total_score += 2
        elif 'LOW' in issue: total_score += 1
    
    # ë©”íƒ€ë°ì´í„° ì´ìŠˆ ì ìˆ˜ í•©ì‚°
    for issue in metadata_issues:
        if 'ê³ ì¶© ìƒë‹´' in issue: total_score += 3
        elif 'í•´ê²° ë¶ˆê°€' in issue: total_score += 3
        # ... ê¸°íƒ€ ë©”íƒ€ë°ì´í„° ì ìˆ˜
    
    return min(total_score, 10)
```

**ì¥ì **: 
- êµ¬í˜„ì´ ê°„ë‹¨í•˜ê³  ì´í•´í•˜ê¸° ì‰¬ì›€
- ì ìˆ˜ ê³„ì‚°ì´ íˆ¬ëª…í•¨
- ì˜ˆì¸¡ ê°€ëŠ¥í•œ ê²°ê³¼

**ë‹¨ì **: 
- ì—¬ëŸ¬ ìš”ì¸ì´ ì¤‘ì²©ë˜ì–´ë„ ì„ í˜•ì ìœ¼ë¡œë§Œ ì¦ê°€
- ì‹¬ê°í•œ ì¼€ì´ìŠ¤ì™€ ê²½ë¯¸í•œ ì¼€ì´ìŠ¤ì˜ êµ¬ë¶„ì´ ì•½í•¨

---

#### ë°©ë²•ë¡  2: ì§€ìˆ˜ì  ì¦í­ ë°©ì‹ (Exponential Amplification)

í•©ì‚°ëœ ì ìˆ˜ë¥¼ ê±°ë“­ì œê³±í•˜ì—¬ ì¦í­í•˜ëŠ” ë°©ì‹ìœ¼ë¡œ, ì—¬ëŸ¬ ì•…ì„± ìš”ì¸ì´ ë™ì‹œì— ê°ì§€ë  ë•Œ ìœ„í—˜ë„ë¥¼ ë” í¬ê²Œ ë°˜ì˜í•©ë‹ˆë‹¤.

```python
def calculate_risk_exponential(baseline_issues, metadata_issues, power=1.5):
    """ì§€ìˆ˜ì  ì¦í­ ë°©ì‹"""
    linear_sum = calculate_risk_linear(baseline_issues, metadata_issues)
    
    # ì§€ìˆ˜ì  ì¦í­
    amplified_score = linear_sum ** power
    
    # 0-10 ìŠ¤ì¼€ì¼ ì¡°ì •
    final_score = min(round(amplified_score), 10)
    
    # ìµœì†Œ 1ì  ë³´ì¥ (ì•…ì„± ìš”ì¸ ê°ì§€ ì‹œ)
    if linear_sum > 0 and final_score == 0:
        final_score = 1
    
    return final_score
```

**ì¥ì **: 
- ì—¬ëŸ¬ ì•…ì„± ìš”ì¸ ì¤‘ì²© ì‹œ ìœ„í—˜ë„ê°€ í¬ê²Œ ì¦ê°€
- ì‹¬ê°í•œ ì¼€ì´ìŠ¤ë¥¼ ë” ëª…í™•íˆ êµ¬ë¶„
- `power` íŒŒë¼ë¯¸í„°ë¡œ ì¦í­ ê°•ë„ ì¡°ì ˆ ê°€ëŠ¥

**ë‹¨ì **: 
- ì ìˆ˜ ë³€í™”ê°€ ë¹„ì„ í˜•ì ì´ë¼ ì˜ˆì¸¡ì´ ì–´ë ¤ì›€
- `power` ê°’ì— ë”°ë¼ ê²°ê³¼ê°€ í¬ê²Œ ë‹¬ë¼ì§
- ê²½ë¯¸í•œ ì¼€ì´ìŠ¤ë„ ê³¼ë„í•˜ê²Œ ì¦í­ë  ìˆ˜ ìˆìŒ

---

#### ë°©ë²•ë¡  3: ê°€ì¤‘ í‰ê·  ë°©ì‹ (Weighted Average)

Baseline ì ìˆ˜ì™€ ë©”íƒ€ë°ì´í„° ì ìˆ˜ì— ê°€ì¤‘ì¹˜ë¥¼ ì ìš©í•˜ì—¬ í‰ê· ì„ ê³„ì‚°í•˜ëŠ” ë°©ì‹ì…ë‹ˆë‹¤.

```python
def calculate_risk_weighted(baseline_score, metadata_score, 
                           baseline_weight=0.7, metadata_weight=0.3):
    """ê°€ì¤‘ í‰ê·  ë°©ì‹"""
    weighted_score = (baseline_score * baseline_weight + 
                     metadata_score * metadata_weight)
    
    return min(round(weighted_score), 10)
```

**ì¥ì **: 
- Baselineê³¼ ë©”íƒ€ë°ì´í„°ì˜ ì¤‘ìš”ë„ë¥¼ ì¡°ì ˆ ê°€ëŠ¥
- ë‘ ì ìˆ˜ ì†ŒìŠ¤ì˜ ê· í˜•ì„ ë§ì¶œ ìˆ˜ ìˆìŒ
- ë„ë©”ì¸ íŠ¹ì„±ì— ë§ê²Œ ê°€ì¤‘ì¹˜ ì¡°ì • ê°€ëŠ¥

**ë‹¨ì **: 
- ê°€ì¤‘ì¹˜ ì„¤ì •ì´ ì£¼ê´€ì ì¼ ìˆ˜ ìˆìŒ
- ìµœëŒ€ê°’ë³´ë‹¤ ë‚®ì€ ì ìˆ˜ê°€ ë‚˜ì˜¬ ìˆ˜ ìˆìŒ

---

#### ë°©ë²•ë¡  4: ìµœëŒ€ê°’ ë°©ì‹ (Maximum)

Baseline ì ìˆ˜ì™€ ë©”íƒ€ë°ì´í„° ì ìˆ˜ ì¤‘ ë” ë†’ì€ ê°’ì„ ì„ íƒí•˜ëŠ” ë°©ì‹ì…ë‹ˆë‹¤.

```python
def calculate_risk_maximum(baseline_score, metadata_score):
    """ìµœëŒ€ê°’ ë°©ì‹"""
    return max(baseline_score, metadata_score)
```

**ì¥ì **: 
- ê°€ì¥ ì‹¬ê°í•œ ìš”ì¸ì„ ìš°ì„  ë°˜ì˜
- êµ¬í˜„ì´ ë§¤ìš° ê°„ë‹¨
- ë³´ìˆ˜ì ì¸ ìœ„í—˜ë„ í‰ê°€

**ë‹¨ì **: 
- ì—¬ëŸ¬ ìš”ì¸ì˜ ì¤‘ì²© íš¨ê³¼ë¥¼ ë°˜ì˜í•˜ì§€ ëª»í•¨
- ì ìˆ˜ê°€ ë‚®ê²Œ ë‚˜ì˜¬ ìˆ˜ ìˆìŒ

---

#### ë°©ë²•ë¡  5: ë¡œê·¸ ìŠ¤ì¼€ì¼ ë°©ì‹ (Logarithmic Scale)

ì„ í˜• í•©ì‚° ì ìˆ˜ì— ë¡œê·¸ í•¨ìˆ˜ë¥¼ ì ìš©í•˜ì—¬ ì ìˆ˜ ì¦ê°€ë¥¼ ì™„í™”í•˜ëŠ” ë°©ì‹ì…ë‹ˆë‹¤.

```python
import math

def calculate_risk_logarithmic(baseline_issues, metadata_issues, base=2):
    """ë¡œê·¸ ìŠ¤ì¼€ì¼ ë°©ì‹"""
    linear_sum = calculate_risk_linear(baseline_issues, metadata_issues)
    
    if linear_sum == 0:
        return 0
    
    # ë¡œê·¸ ìŠ¤ì¼€ì¼ ì ìš©
    log_score = math.log(linear_sum + 1, base) * (10 / math.log(11, base))
    
    return min(round(log_score), 10)
```

**ì¥ì **: 
- ì ìˆ˜ ì¦ê°€ê°€ ì™„ë§Œí•¨
- ë‚®ì€ ì ìˆ˜ êµ¬ê°„ì—ì„œ ì„¸ë°€í•œ êµ¬ë¶„ ê°€ëŠ¥
- ë†’ì€ ì ìˆ˜ êµ¬ê°„ì—ì„œ í¬í™” íš¨ê³¼

**ë‹¨ì **: 
- ë†’ì€ ìœ„í—˜ë„ ì¼€ì´ìŠ¤ì˜ êµ¬ë¶„ì´ ì•½í•¨
- ë¡œê·¸ í•¨ìˆ˜ì˜ íŠ¹ì„±ìƒ ì§ê´€ì ì´ì§€ ì•ŠìŒ

---

#### ë°©ë²•ë¡  6: ë‹¨ê³„ë³„ ì ìˆ˜ ë°©ì‹ (Tiered Scoring)

ì‹¬ê°ë„ ë ˆë²¨ì— ë”°ë¼ ë‹¨ê³„ë³„ë¡œ ì ìˆ˜ë¥¼ ë¶€ì—¬í•˜ëŠ” ë°©ì‹ì…ë‹ˆë‹¤.

```python
def calculate_risk_tiered(baseline_issues, metadata_issues):
    """ë‹¨ê³„ë³„ ì ìˆ˜ ë°©ì‹"""
    # ê°€ì¥ ë†’ì€ ì‹¬ê°ë„ ë ˆë²¨ ì°¾ê¸°
    max_severity = 0
    
    for issue in baseline_issues:
        if 'CRITICAL' in issue:
            max_severity = max(max_severity, 4)
        elif 'HIGH' in issue:
            max_severity = max(max_severity, 3)
        elif 'MEDIUM' in issue:
            max_severity = max(max_severity, 2)
        elif 'LOW' in issue:
            max_severity = max(max_severity, 1)
    
    # ì´ìŠˆ ê°œìˆ˜ì— ë”°ë¥¸ ë³´ë„ˆìŠ¤ ì ìˆ˜
    issue_count = len(baseline_issues) + len(metadata_issues)
    bonus = min(issue_count - 1, 2)  # ìµœëŒ€ 2ì  ë³´ë„ˆìŠ¤
    
    return min(max_severity + bonus, 10)
```

**ì¥ì **: 
- ê°€ì¥ ì‹¬ê°í•œ ìš”ì¸ì„ ìš°ì„  ë°˜ì˜
- ì—¬ëŸ¬ ìš”ì¸ ì¤‘ì²© ì‹œ ë³´ë„ˆìŠ¤ ì ìˆ˜ ë¶€ì—¬
- ì§ê´€ì ì´ê³  í•´ì„í•˜ê¸° ì‰¬ì›€

**ë‹¨ì **: 
- ë³´ë„ˆìŠ¤ ì ìˆ˜ ê³„ì‚° ë°©ì‹ì´ ì£¼ê´€ì 
- ì„¸ë°€í•œ ì ìˆ˜ êµ¬ë¶„ì´ ì–´ë ¤ì›€

---

#### ë°©ë²•ë¡  7: í•˜ì´ë¸Œë¦¬ë“œ ë°©ì‹ (Hybrid)

ì—¬ëŸ¬ ë°©ë²•ë¡ ì„ ì¡°í•©í•˜ì—¬ ì‚¬ìš©í•˜ëŠ” ë°©ì‹ì…ë‹ˆë‹¤.

```python
def calculate_risk_hybrid(baseline_issues, metadata_issues, method='adaptive'):
    """í•˜ì´ë¸Œë¦¬ë“œ ë°©ì‹"""
    baseline_score = calculate_risk_linear(baseline_issues, [])
    metadata_score = calculate_risk_linear([], metadata_issues)
    
    if method == 'adaptive':
        # ì ìˆ˜ê°€ ë‚®ìœ¼ë©´ ì„ í˜•, ë†’ìœ¼ë©´ ì§€ìˆ˜ì  ì¦í­
        if baseline_score + metadata_score < 5:
            return calculate_risk_linear(baseline_issues, metadata_issues)
        else:
            return calculate_risk_exponential(baseline_issues, metadata_issues)
    
    elif method == 'weighted_max':
        # ê°€ì¤‘ í‰ê· ê³¼ ìµœëŒ€ê°’ì˜ í‰ê· 
        weighted = calculate_risk_weighted(baseline_score, metadata_score)
        maximum = calculate_risk_maximum(baseline_score, metadata_score)
        return round((weighted + maximum) / 2)
```

**ì¥ì **: 
- ì—¬ëŸ¬ ë°©ë²•ë¡ ì˜ ì¥ì ì„ ê²°í•©
- ìƒí™©ì— ë”°ë¼ ì ì‘ì ìœ¼ë¡œ ê³„ì‚°
- ìœ ì—°í•œ ì ìˆ˜ ì‚°ì •

**ë‹¨ì **: 
- êµ¬í˜„ì´ ë³µì¡í•¨
- ê²°ê³¼ í•´ì„ì´ ì–´ë ¤ìš¸ ìˆ˜ ìˆìŒ

---

#### ë°©ë²•ë¡  ì„ íƒ ê°€ì´ë“œ

| ë°©ë²•ë¡  | ì í•©í•œ ìƒí™© | ê¶Œì¥ íŒŒë¼ë¯¸í„° |
|--------|-----------|--------------|
| ì„ í˜• í•©ì‚° | ë‹¨ìˆœí•˜ê³  íˆ¬ëª…í•œ ì ìˆ˜ ì‚°ì • í•„ìš” | - |
| ì§€ìˆ˜ì  ì¦í­ | ì—¬ëŸ¬ ìš”ì¸ ì¤‘ì²© ì‹œ ìœ„í—˜ë„ ê°•ì¡° | power=1.5~2.0 |
| ê°€ì¤‘ í‰ê·  | Baselineê³¼ ë©”íƒ€ë°ì´í„° ê· í˜• í•„ìš” | baseline_weight=0.6~0.8 |
| ìµœëŒ€ê°’ | ê°€ì¥ ì‹¬ê°í•œ ìš”ì¸ë§Œ ë°˜ì˜ | - |
| ë¡œê·¸ ìŠ¤ì¼€ì¼ | ë‚®ì€ ì ìˆ˜ êµ¬ê°„ ì„¸ë°€í•œ êµ¬ë¶„ í•„ìš” | base=2~3 |
| ë‹¨ê³„ë³„ ì ìˆ˜ | ì‹¬ê°ë„ ë ˆë²¨ ì¤‘ì‹¬ í‰ê°€ | bonus_max=2~3 |
| í•˜ì´ë¸Œë¦¬ë“œ | ë³µì¡í•œ ìš”êµ¬ì‚¬í•­, ì ì‘ì  í‰ê°€ | method='adaptive' |

#### ì‹¤ì œ êµ¬í˜„ ê¶Œì¥ì‚¬í•­

1. **ì´ˆê¸° êµ¬í˜„**: ì„ í˜• í•©ì‚° ë°©ì‹ìœ¼ë¡œ ì‹œì‘í•˜ì—¬ ì ìˆ˜ ë¶„í¬ í™•ì¸
2. **ë°ì´í„° ë¶„ì„**: ì‹¤ì œ ë°ì´í„°ì—ì„œ ì ìˆ˜ ë¶„í¬ì™€ ë ˆë²¨ ë¶„ë¦¬ë„ í™•ì¸
3. **ë°©ë²•ë¡  ì„ íƒ**: ë°ì´í„° íŠ¹ì„±ê³¼ ë¹„ì¦ˆë‹ˆìŠ¤ ìš”êµ¬ì‚¬í•­ì— ë§ëŠ” ë°©ë²•ë¡  ì„ íƒ
4. **íŒŒë¼ë¯¸í„° íŠœë‹**: ì„ íƒí•œ ë°©ë²•ë¡ ì˜ íŒŒë¼ë¯¸í„°ë¥¼ ê²€ì¦ ë°ì´í„°ë¡œ íŠœë‹
5. **ì§€ì†ì  ê°œì„ **: ìš´ì˜ ë°ì´í„°ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë°©ë²•ë¡  ê°œì„ 

**ì°¸ê³ **: í˜„ì¬ ë…¸íŠ¸ë¶ êµ¬í˜„ì—ì„œëŠ” ì§€ìˆ˜ì  ì¦í­ ë°©ì‹ì„ ì‚¬ìš©í•˜ê³  ìˆìœ¼ë©°, `amplification_power=1.5`ë¡œ ì„¤ì •ë˜ì–´ ìˆìŠµë‹ˆë‹¤. í•„ìš”ì— ë”°ë¼ ë‹¤ë¥¸ ë°©ë²•ë¡ ìœ¼ë¡œ ë³€ê²½í•˜ê±°ë‚˜ í•˜ì´ë¸Œë¦¬ë“œ ë°©ì‹ìœ¼ë¡œ í™•ì¥í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

### í•˜ì´ë¸Œë¦¬ë“œ í†µí•© (Baseline + KoBERT)

```python
# hybrid_classifier.py

from classification_criteria import ClassificationCriteria
from risk_based_classifier import RiskScoreClassifier
from transformers import BertTokenizer, BertForSequenceClassification
import torch

class HybridComplaintClassifier:
    """Baseline + KoBERT í•˜ì´ë¸Œë¦¬ë“œ ë¶„ë¥˜ê¸°"""
    
    def __init__(self, kobert_model_path: str = None, kobert_model=None, tokenizer=None):
        # Baseline ê·œì¹™ ì—”ì§„
        self.baseline = ClassificationCriteria()
        self.risk_classifier = RiskScoreClassifier()
        
        # KoBERT ëª¨ë¸ (ì„ íƒì‚¬í•­)
        self.kobert_model = kobert_model
        self.tokenizer = tokenizer
        self.use_kobert = kobert_model is not None and tokenizer is not None
        
        if kobert_model_path and not self.use_kobert:
            try:
                self.tokenizer = BertTokenizer.from_pretrained(kobert_model_path)
                self.kobert_model = BertForSequenceClassification.from_pretrained(kobert_model_path)
                self.kobert_model.eval()
                self.use_kobert = True
            except Exception as e:
                print(f"âš ï¸ KoBERT ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
                self.use_kobert = False
    
    def classify(self, text: str, session_context: List[str] = None,
                 metadata: Optional[ConsultationMetadata] = None,
                 use_baseline_threshold: float = 0.8):
        """
        í•˜ì´ë¸Œë¦¬ë“œ ë¶„ë¥˜
        
        Args:
            text: ë¶„ì„í•  í…ìŠ¤íŠ¸
            session_context: ì„¸ì…˜ ë§¥ë½
            metadata: ìƒë‹´ ë©”íƒ€ë°ì´í„°
            use_baseline_threshold: Baseline ì‹ ë¢°ë„ê°€ ì´ ê°’ ì´ìƒì´ë©´ Baseline ìš°ì„  ì‚¬ìš©
        """
        # 1. Baseline ë¶„ë¥˜
        baseline_results = self.baseline.classify_text(text, session_context)
        risk_result = self.risk_classifier.classify(text, session_context, metadata)
        baseline_max_confidence = max([r.confidence for r in baseline_results], default=0.0)
        
        # 2. Baseline ì‹ ë¢°ë„ê°€ ë†’ìœ¼ë©´ Baseline ê²°ê³¼ ì‚¬ìš©
        if baseline_max_confidence >= use_baseline_threshold:
            return {
                'method': 'baseline',
                'risk_score': risk_result.risk_score,
                'risk_level': risk_result.risk_level.name,
                'labels': [r.category.value for r in baseline_results if r.severity != ComplaintSeverity.NORMAL],
                'confidence': baseline_max_confidence,
                'recommendation': risk_result.recommendation
            }
        
        # 3. ê·¸ë ‡ì§€ ì•Šìœ¼ë©´ KoBERT ì‚¬ìš©
        if self.use_kobert:
            kobert_labels, kobert_probs = self._classify_with_kobert(text)
            
            # 4. ë‘ ê²°ê³¼ í†µí•© (Ensemble)
            ensemble_labels = set()
            
            # KoBERT ê²°ê³¼ ì¶”ê°€
            for label in kobert_labels:
                ensemble_labels.add(label)
            
            # Baselineì˜ ë†’ì€ ì‹ ë¢°ë„ ê²°ê³¼ ì¶”ê°€
            for result in baseline_results:
                if result.confidence > 0.7:
                    ensemble_labels.add(result.category.value)
            
            return {
                'method': 'hybrid',
                'risk_score': risk_result.risk_score,
                'risk_level': risk_result.risk_level.name,
                'labels': list(ensemble_labels),
                'baseline_labels': [r.category.value for r in baseline_results if r.severity != ComplaintSeverity.NORMAL],
                'kobert_labels': kobert_labels,
                'kobert_probs': kobert_probs,
                'confidence': max(baseline_max_confidence, max(kobert_probs.values()) if kobert_probs else 0.0),
                'recommendation': risk_result.recommendation
            }
        else:
            # KoBERTê°€ ì—†ìœ¼ë©´ Baselineë§Œ ì‚¬ìš©
            return {
                'method': 'baseline',
                'risk_score': risk_result.risk_score,
                'risk_level': risk_result.risk_level.name,
                'labels': [r.category.value for r in baseline_results if r.severity != ComplaintSeverity.NORMAL],
                'confidence': baseline_max_confidence,
                'recommendation': risk_result.recommendation
            }
    
    def _classify_with_kobert(self, text: str, threshold: float = 0.5):
        """KoBERTë¡œ ë¶„ë¥˜"""
        inputs = self.tokenizer(
            text,
            return_tensors='pt',
            truncation=True,
            padding=True,
            max_length=128
        )
        
        with torch.no_grad():
            outputs = self.kobert_model(**inputs)
            logits = outputs.logits
            probs = torch.sigmoid(logits).squeeze().numpy()
        
        # ì„ê³„ê°’ìœ¼ë¡œ ì´ì§„ ë¶„ë¥˜
        predictions = (probs > threshold).astype(int)
        
        # ë¼ë²¨ ë§¤í•‘ ì—­ë³€í™˜
        predicted_labels = [REVERSE_LABEL_MAPPING[i] for i in range(len(predictions))
                           if predictions[i] == 1]
        
        # í™•ë¥  ë”•ì…”ë„ˆë¦¬
        prob_dict = {REVERSE_LABEL_MAPPING[i]: float(probs[i])
                    for i in range(len(probs))}
        
        return predicted_labels, prob_dict
```

---

## ğŸ“ ë°ì´í„°ì…‹ ì¤€ë¹„ ì²´í¬ë¦¬ìŠ¤íŠ¸

### í•„ìˆ˜ í•­ëª©
- [ ] ì›ë³¸ ë°ì´í„° ìˆ˜ì§‘ (CSV/JSON)
- [ ] í…ìŠ¤íŠ¸ ì •ì œ (PII ë§ˆìŠ¤í‚¹, íŠ¹ìˆ˜ë¬¸ì ì •ê·œí™”)
- [ ] ë¼ë²¨ë§ (ë‹¤ì¤‘ ë¼ë²¨ ì§€ì›)
- [ ] ì„¸ì…˜ ID í• ë‹¹
- [ ] ë°ì´í„° ë¶„í•  (ì„¸ì…˜ ëˆ„ìˆ˜ ë°©ì§€)
- [ ] í´ë˜ìŠ¤ ë¶ˆê· í˜• í™•ì¸ ë° ì²˜ë¦¬

### ê¶Œì¥ í•­ëª©
- [ ] Baselineìœ¼ë¡œ ìë™ ë¼ë²¨ë§ (ê²€ì¦ìš©)
- [ ] ë°ì´í„° í’ˆì§ˆ ê²€ì¦
- [ ] í†µê³„ ë¶„ì„ (ë¼ë²¨ ë¶„í¬, í…ìŠ¤íŠ¸ ê¸¸ì´ ë“±)
- [ ] ìƒ˜í”Œ ë°ì´í„° ì‹œê°í™”

---

## ğŸš€ ë‹¤ìŒ ë‹¨ê³„

1. **ë°ì´í„°ì…‹ í™•ì¸**: ì œê³µëœ ë°ì´í„°ì…‹ êµ¬ì¡° íŒŒì•…
2. **ë°ì´í„° ì „ì²˜ë¦¬**: ìœ„ì˜ í†µí•© ê³¼ì • ì ìš©
3. **Baseline ê²€ì¦**: í˜„ì¬ ê·œì¹™ ì—”ì§„ìœ¼ë¡œ ìƒ˜í”Œ í…ŒìŠ¤íŠ¸
4. **KoBERT Fine-tuning**: ë°ì´í„° ì¤€ë¹„ ì™„ë£Œ í›„ í•™ìŠµ
5. **í•˜ì´ë¸Œë¦¬ë“œ í†µí•©**: Baseline + KoBERT ê²°í•©
6. **í‰ê°€ ë° ê°œì„ **: ì„±ëŠ¥ ì¸¡ì • ë° ë°˜ë³µ ê°œì„ 


