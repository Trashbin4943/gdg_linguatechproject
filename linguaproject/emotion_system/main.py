from stt import transcribe_audio
from text_emotion import analyze_text_emotion
from audio_features import extract_features
from audio_models import build_lstm, predict_audio_emotion
from ensemble import combine_emotions
from response_generator import generate_response

'''
ì „ì²´ ë©”ì¸ í•¨ìˆ˜ì…ë‹ˆë‹¤.
'''

emotion_map = {
'ë¶ˆë§Œ': 0, 'ë¶„ë…¸': 1, 'ë¶ˆì•ˆ': 2, 'ì¤‘ë¦½': 3,
'ê°ì‚¬': 4, 'ìš”ì²­': 5, 'í˜¼ë€': 6
}

audio_path = "audio_folder/sample.wav"
text = transcribe_audio(audio_path)
text_emotion = analyze_text_emotion(text)
features = extract_features(audio_path)
audio_model = build_lstm(input_dim=features.shape[0], num_classes=12)
audio_emotion = predict_audio_emotion(audio_model, features)
final_emotion = combine_emotions(text_emotion, audio_emotion)
response = generate_response(final_emotion, text)

print("ğŸ—£ï¸ ì‚¬ìš©ì ë°œí™”:", text)
print("ğŸ¯ í…ìŠ¤íŠ¸ ê°ì •:", emotion_map[text_emotion])
print("ğŸ”Š ìŒí–¥ ê°ì •:", emotion_map[audio_emotion])
print("âœ… ìµœì¢… ê°ì •:", emotion_map[final_emotion])
print("ğŸ’¬ ìƒë‹´ì‚¬ ì‘ë‹µ:", response)
